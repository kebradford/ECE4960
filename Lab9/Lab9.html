<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <title>Lab 9</title>
    <style>
      body{
        padding: 0 80px;}
    </style>
</head>
  <body>
    <div id= "header">
      <center><h1> Lab 9 - Grid Localization using Bayes Filter - Real</h1></center>
    </div>
    <div id = "navbar">
      <a href="https://kebradford.github.io/ECE4960"><button class="btn"><i class="fa fa-home"></i> Back to Home Page</button></a>
    </div>
    <br>
    <br>
    <center><h2>Objective </h2></center>
    
      <p>Offline Localization</p>

    <p>
        <br>
        I started with offline localization. I copied over my map data from Lab7. 
        <br>
      <img src="map_setup.PNG">
    <br>
    I then had to update the mapper call to fit my bounds. I updated min and max x/y as well as the cell size to fit my grid, which resulted in a mapper call of 

     <code><pre>
  mapper = Mapper(min_x=-1.85, max_x=.2, min_y=-.79, max_y=0, min_a=-180, max_a=180,
                cell_size_x=0.1025, cell_size_y=0.0395, cell_size_a=20,
                max_cells_x=20, max_cells_y=20, max_cells_a=18,
                ray_length=3, lines=[start_points, end_points], obs_per_cell=18, 
                robot=robot)
    </pre></code>
    <br>
    Then, I ran a scan at a given point (-.55, -.37) on my floor. I paired down the data to 18 points evenly spaced by 20 degrees and wrote that data to loc.obs_range_data for the Bayes filter to try and guess where I was. It did a fairly good job, a few cells off, but my cells were very very small to begin with. 

    <br>
      <img src="offline_close.PNG"><img src="offline_compare2.PNG">
    <br>

    These were both fairly close to the two locations I had the robot scan from, with the actual location in green and the projected in yellow. 
    <br>
      <img src="offline_belbar.PNG"><img src="offline_belbar2.PNG">
    <br>
    The confidence was not particularly high, likely due to how small the cells were and how the gyro drifts slightly during measurements, but I believe it would be good enough to do mapping later on in labs. 
    <br>
    Next, I moved to online mode. First, I played around with Bluetooth ensuring I could also recieve bluetooth commands on the artemis, which I accomplished using the await theRobot.sendMessage() function. This ended up being fairly easy to implement, so I was able to send a command to tell the robot to perform a scan. I wrote this data to a file, where I paired down the data into the closest increments to 20 degrees as possible. 

<code><pre> 
  if(unpack("IQ", data)[0] >= nextIndex + 20):
                    print(unpack("IQ", data))
                    
                    f = open(scan.txt", "a")
                    f.write(str(unpack("IQ", data)))
                    f.write("\n")
                    f.close()
                    
                    nextIndex +=20
    </pre></code>

an example of the output can be seen here : 
<code><pre>
(0, 28)
(22, 519)
(44, 240)
(67, 635)
(80, 743)
(101, 476)
(122, 351)
(141, 208)
(160, 106)
(180, 277)
(202, 250)
(223, 278)
(242, 260)
(261, 188)
(282, 472)
(309, 296)
(324, 86)
(343, 454)
(360, 329)
    </pre></code>
Although they are not perfectly 20 degrees apart, they are fairly close. I can then read this file into Jupyter lab when a scan is requested. The scan stops at 360 degrees. 

<video width="320" height="240" controls>
      <source src="scan.mp4" type="video/mp4">
      </video>
    <h3></h3>
The next challenge was getting the asyncronous bluetooth script to integrate with the Jupyter notebook. I struggled with this for a while until Alex posted his video with suggestions on how to integrate the bluetooth code into Jupyter notebook. I brought all the files into the jupyter notebook, created a robot class to reference, and imported main directly into jupyter script. I added a short bit of code to pull the robot into the script 
<code><pre>
loop = asyncio.get_event_loop()
asyncio.gather(robotTest(loop))
theRobot = theRobotHolder.getRobot()
    </pre></code>
    After this, and toying around with bluetooth settings in my host OS, I finally got the two communicating 

    <br>
      <img src="bluetoothcomms.PNG">
    <br>

    </p>
    <br>
    
  
 
   <a href="#header"><button class="btn"><i class="fa fa-arrow-up"></i>Return to top</button></a>
   </body>
</html>
