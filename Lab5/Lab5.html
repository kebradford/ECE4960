<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <title>Lab 5</title>
    <style>
      body{
        padding: 0 80px;}
    </style>
</head>
  <body>
    <div id= "header">
      <center><h1> Lab 5 - Obstacle Avoidance</h1></center>
    </div>
    <div id = "navbar">
      <a href="https://kebradford.github.io/ECE4960"><button class="btn"><i class="fa fa-home"></i> Back to Home Page</button></a>
    </div>
    <br>
    <br>
    <center><h2>Objective </h2></center>
    
      <p>Perform obstacle avoidance with the physical robot using TOF and proximity sensors, on the virtual robot using laser range finder</p>
      <br>
    <br>
    <center><h2>Materials Used</h2></center>
       <ul>
         <li>1 RC Car</li>
         <li>1 Artemis Nano</li>
         <li>1 USB A/C Cable</li>
         <li>2 batteries (car battery, board battery)</li>
         <li>1 Sparkfun motor driver</li>
         <li>3 Qwiic connectors</li>
         <li>2 screwdrivers (flathead and phillips)</li>
         <li>1 wire stripper/cutter</li>
         <li>1 TOF sensor</li>
         <li>1 proximity sensor</li>
       </ul>
    <br>
    <br>


    <center><h2>Procedure</h2></center>
    <p>
      <h3>5A: Obstacle Avoidance on Physical Robot</h3>
      <br><br>
      First, in order to perform obstacle avoidance on the physical robot, I had to parameterize the two sensors: the proximity sensor and the TOF sensor. 
      <br><br>
      <h4>Proximity Sensor</h4>

      <br><br>
      The proximity sensor used was the SparkFun VCNL4040 Proximity Sensor. First, I set up the Arduino library, hooked up the sensor via Qwicc connect to the Artemis Nano, and used the example Wire.c to scan the i2c bus for the sensor. 
      <br>
      <img src="i2c1.PNG" alt="i2c1" width="320" height="240">
      <br>

      After that, I used the example code in the library AllReadings to map the sensor readings to actual distances. I used 4 objects: 
      <ul>
         <li>1 Brown Box</li>
         <li>1 Box with White Paper taped to it</li>
         <li>1 red Sparkfun box</li>
         <li>1 black PCB </li>
       </ul>

      <br>
      <img src="brown_box.jpg" alt="brown_box" width="240" height="320"><img src="red_box.jpg" alt="red_box" width="240" height="320">
      <br>

       Noteably, these objects were different colors but also different sizes. The white and brown objects were much larger than the red and brown, and therefore likely blocked out more light. I took measurements with the sensors at several distances to collect meaningful data about the values the sensor would report. I took this data with primarily ambient light from my window. The white paper reflected a lot more light than anything else, as expcted, hence the particularly high value for that data set. 

      <br>
      <img src="ambient_light.PNG" alt="ambient_light" width="700" height="500"><img src="white_level.PNG" alt="white_level" width="700" height="500">
      <br>


      <br><br> 
      <h4>Time of Flight Sensor</h4>
      <br><br>
      First, I hooked up the TOF sensor to the Artemis and scanned the i2c bus, finding the address to be x29 rather than the expected x52. This may be something with dropping the last bit of the address, because 52 and 29 converted to hex differ by only the last bit, which is a zero. 

      <br>
      <img src="i2c2.PNG" alt="i2c2" width="320" height="240">
      <br>
      Then, I loaded up the example code to read some data points, and noticed that really close to the sensor, the readings would zero out far before I was actually at the sensor. Therefore, I ran the calibration code with the grey target 5 times, getting offsets of 30, 31, 33, 29, and 31 mm respectively. Therefore, I chose an offset of 31mm. This worked a lot better after again testing with the sensor code. 
      <br><br>
      We found in lab 3 that the robot had an average fast velocity of 2.4 m/s and a stopping distance of around 13 cm, so the robot will need at least .054 seconds to stop or turn around once it encounters an obstacle. Therefore, the total of the intermeasurement period plus the timing budget is required to be 54 ms. The intermeasurement period must be larger than the timing budget, else nothing else can happen in the code because the next measurement will start immediately. Therefore, I would set the intermeasurement period to be 35ms and the timing budget to be 20ms (the minimum). 
      <br><br>
      In order to set the distance mode, I looked at the robot's speed and acceleration. The benefit of the short mode was "better ambient immunity" and of long was "maximum distance". Since the robot can move and accelerate/deccelerate rather quickly, I chose the short mode to better deal with a change in ambient light conditions, as I would be testing this in a place where night vs daytime light makes a big difference. 
      <br><br>
      Next I ran the StatusAndRate example to see what the failure modes could look like when the robot was moving quickly. To test this, I uploaded the code then waved my hand rapidly and chaoticly in front of the sensor, for science. I found that there were occasional errors in the read status such as "Wrapped target fail" and "Signal fail". In the future of motion implementation, I will have to check for these errors and probably disregard certain read test points. 
      <br>
      <img src="failure_modes.PNG" alt="i2c2">
      <br>




      <br><br>
      <h3>5B: Obstacle Avoidance in Simulation</h3>
      <br><br>
      In the VM, I ran the lab5 set up code then opened the simulator and jupyter notebook as described in the lab manual. To do obstacle avoidance, I had to use the laser range finder data mounted to the front of the virtual robot. 
      <br>
      Using a simple if/else loop, I measured whether there was a wall in front of the robot, starting with an offset of .5. If there was a wall, I would stop and turn the robot for .1s. If not, I would set a positive velocity for .1 seconds. 
      <br>
      After testing this, I found that the major issue occurred when the robot was moving not directly at a wall but not perfectly parallel. The laser range finder had a rather narrow beam, so if a wall was around 10 degrees out of parallel with the robot, the robot would not see it but would crash into it. 
      <br>
      In order to address this issue, I increased the distance at which the robot would determine there was a wall in front of it to .7. Essentially, then the robot could measure a hypotenuse rather than the direct perpendicular distance to the wall. This meant the robot couldn't get as close to the wall, but it heavily reduced the number of collisions. 
      <br>
      <video width="320" height="240" controls>
      <source src="obstacle_avoidance.mp4" type="video/mp4">
      </video>
      <br>
      I continued tweaking the values for how long the robot should delay and how long it should turn for, and at what speeds. I found a combination of moving at .5 linear velocity for movement and 2 angular velocity for turns to be the best balance of speed to obstacle avoidance. In reality, I hope that the robot can glance off surfaces at a low enough angle without causing it to stop or flip, although this would not be ideal. 
      There is a delay of .1s between updates of this function, which will be adjusted as other functions need to occur between measurements. Once the robot is performing other tasks, those other tasks will take some finite amount of time, and that will have to cocur between measurements. 
      <br>
      The robot could move faster if this delay was decreased. 
      As seen in the video, the robot gets very very close, almost touching the wall at one point, before it deflects, not resulting in a collision. 
      In order to minimize crashes we could move slower, take measurements of wall distance more often, or set a larger distance required between the robot and an obstacle, which would work well in a  low-obstacle environment. 

      <code><pre>
      def perform_obstacle_avoidance(robot):
        while True: 

          robot.set_vel(0, 0)

          if (robot.get_laser_data() < .7): 
            robot.set_vel(0,2)
            time.sleep(.4)

          else: 
            robot.set_vel(.5, 0)
            time.sleep(.1)

      perform_obstacle_avoidance(robot)

      </pre></code>


    </p>
    <br>
    
  
 
   <a href="#header"><button class="btn"><i class="fa fa-arrow-up"></i>Return to top</button></a>
   </body>
</html>
