<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <title>Lab 5</title>
    <style>
      body{
        padding: 0 80px;}
    </style>
</head>
  <body>
    <div id= "header">
      <center><h1> Lab 5 - Obstacle Avoidance</h1></center>
    </div>
    <div id = "navbar">
      <a href="https://kebradford.github.io/ECE4960"><button class="btn"><i class="fa fa-home"></i> Back to Home Page</button></a>
    </div>
    <br>
    <br>
    <center><h2>Objective </h2></center>
    
      <p>Perform obstacle avoidance with the physical robot using TOF and proximity sensors, on the virtual robot using laser range finder</p>
      <br>
    <br>
    <center><h2>Materials Used</h2></center>
       <ul>
         <li>1 RC Car</li>
         <li>1 Artemis Nano</li>
         <li>1 USB A/C Cable</li>
         <li>2 batteries (car battery, board battery)</li>
         <li>1 Sparkfun motor driver</li>
         <li>3 Qwiic connectors</li>
         <li>2 screwdrivers (flathead and phillips)</li>
         <li>1 wire stripper/cutter</li>
         <li>1 TOF sensor</li>
         <li>1 proximity sensor</li>
       </ul>
    <br>
    <br>


    <center><h2>Procedure</h2></center>
    <p>
      <h3>5A: Obstacle Avoidance on Physical Robot</h3>
      <br><br>
      First, in order to perform obstacle avoidance on the physical robot, I had to parameterize the two sensors: the proximity sensor and the TOF sensor. 
      <br><br>
      <h4>Proximity Sensor</h4>

      <br><br>
      The proximity sensor used was the SparkFun VCNL4040 Proximity Sensor. First, I set up the Arduino library, hooked up the sensor via Qwicc connect to the Artemis Nano, and used the example Wire.c to scan the i2c bus for the sensor. 
      <br>
      <img src="i2c1.PNG" alt="i2c1" width="320" height="240">
      <br>

      After that, I used the example code in the library AllReadings to map the sensor readings to actual distances. I used 4 objects: 
      <ul>
         <li>1 Brown Box</li>
         <li>1 Box with White Paper taped to it</li>
         <li>1 red Sparkfun box</li>
         <li>1 black PCB </li>
       </ul>

      <br>
      <img src="brown_box.jpg" alt="brown_box" width="240" height="320"><img src="red_box.jpg" alt="red_box" width="240" height="320">
      <br>

       Noteably, these objects were different colors but also different sizes. The white and brown objects were much larger than the red and brown, and therefore likely blocked out more light. I took measurements with the sensors at several distances to collect meaningful data about the values the sensor would report. I took this data with primarily ambient light from my window. The white paper reflected a lot more light than anything else, as expcted, hence the particularly high value for that data set. 

      <br>
      <img src="ambient_light.PNG" alt="ambient_light" width="700" height="500"><img src="white_level.PNG" alt="white_level" width="700" height="500">
      <br>


      <br><br> 
      <h4>Time of Flight Sensor</h4>
      <br><br>
      First, I hooked up the TOF sensor to the Artemis and scanned the i2c bus, finding the address to be x29 rather than the expected x52. This may be something with dropping the last bit of the address, because 52 and 29 converted to hex differ by only the last bit, which is a zero. 

      <br>
      <img src="i2c2.PNG" alt="i2c2" width="320" height="240">
      <br>
      Then, I loaded up the example code to read some data points, and noticed that really close to the sensor, the readings would zero out far before I was actually at the sensor. Therefore, I ran the calibration code with the grey target 5 times, getting offsets of 30, 31, 33, 29, and 31 mm respectively. Therefore, I chose an offset of 31mm. This worked a lot better after again testing with the sensor code. 
      <br><br>
      We found in lab 3 that the robot had an average fast velocity of 2.4 m/s and a stopping distance of around 13 cm, so the robot will need at least .054 seconds to stop or turn around once it encounters an obstacle. Therefore, the total of the intermeasurement period plus the timing budget is required to be 54 ms. The intermeasurement period must be larger than the timing budget, else nothing else can happen in the code because the next measurement will start immediately. Therefore, I would set the intermeasurement period to be 35ms and the timing budget to be 20ms (the minimum). 
      <br><br>
      In order to set the distance mode, I looked at the robot's speed and acceleration. The benefit of the short mode was "better ambient immunity" and of long was "maximum distance". Since the robot can move and accelerate/deccelerate rather quickly, I chose the short mode to better deal with a change in ambient light conditions, as I would be testing this in a place where night vs daytime light makes a big difference. 
      <br><br>
      Next I ran the StatusAndRate example to see what the failure modes could look like when the robot was moving quickly. To test this, I uploaded the code then waved my hand rapidly and chaoticly in front of the sensor, for science. I found that there were occasional errors in the read status such as "Wrapped target fail" and "Signal fail". In the future of motion implementation, I will have to check for these errors and probably disregard certain read test points. 
      <br>
      <img src="failure_modes.PNG" alt="failure">
      <br>
      After editing the code to have the proper offset, short mode, and timing settings, I ran the data collection code with four objects the same as for the proximity sensor. Overall the data was very accurate after all these settings were adjusted. As expected based on setting short mode, the sensor was unable to read data past 1.3m, which aligns with the data sheet. In order to account for the repeatability of measurements, I took the average of 50 readings to determine the value for a given distance. There was much less of a variation in readings depending on color and size of objects placed in front of the sensor, which makes it a much more attractive option than the proximity sensor. 
      <br>
      On average, sensor inaccuracy was below 5%, however, this was for the average of 50 readings. Individual readings, which will have to be trusted at high speeds, could be off by as much as 30%. 
      <br>
      I also used the micros() function to get the time to read sensor data, and got a bimodial distribution at 12.4 and 14.7ms, likely due to the delay(1) millisecond function while waiting for sensor data. 


      <br>
      <img src="TOF.PNG" alt="TOF">
      <br>

      <br><br>
      <center><h2>Live Obstacle Avoidance</h2></center>
      <br>
      I combined several previous programs to create an obstacle avoidance plan. First, I loaded up the motor code that had previously had the robot running in a straight line. 
      Next, I added in the distance sensor functionality, measuring the distance as described in the previous section. 
      I then created a simple if-else check to see if there was an obstacle in front of the robot, and, if so, to stop spinning the wheels. Here it is on a platform shown by putting my hand in front of it to stop it (this video is kinda bad but every time it stops my hand is going in front of it ) <br>
      <video width="320" height="240" controls>
      <source src="platform_stop.mp4" type="video/mp4">
      </video>
      <br>
      Next I tested it on ground, setting up a 2m test run at a wall. Here is a pic of the setup and the robot running at .75m/s for testing purposes. 
      <br>
      <img src="fakewall.jpg" width="320" height="240" alt="failure">
      <br>
      <video width="320" height="240" controls>
      <source src="slowgoing.mp4" type="video/mp4">
      </video>
      <br>
      Next, I decided to ramp it up! I wanted to find a good balance between speed and distance, and found that a cutoff distance of 250mm and a speed of 160 for the motor driver worked well to stop right up to the wall while going pretty fast before that. 
      In this video, the robot is going about 2 m/s and stopping right before the wall 
      <br>
      <video width="320" height="240" controls>
      <source src="robot_fast.mp4" type="video/mp4">
      </video>
      <br>
      At first I was trying to just stop the motors when it reached what it determined to be an obstacle, but found that on my hardwood floors it would sometimes just slide right into the obstacle. Therefore, I ended up turning the motors in the opposite direction for .25 seconds before stopping to really break hard. 







      <br><br>
      <h3>5B: Obstacle Avoidance in Simulation</h3>
      <br><br>
      In the VM, I ran the lab5 set up code then opened the simulator and jupyter notebook as described in the lab manual. To do obstacle avoidance, I had to use the laser range finder data mounted to the front of the virtual robot. 
      <br>
      Using a simple if/else loop, I measured whether there was a wall in front of the robot, starting with an offset of .5. If there was a wall, I would stop and turn the robot for .1s. If not, I would set a positive velocity for .1 seconds. 
      <br>
      After testing this, I found that the major issue occurred when the robot was moving not directly at a wall but not perfectly parallel. The laser range finder had a rather narrow beam, so if a wall was around 10 degrees out of parallel with the robot, the robot would not see it but would crash into it. 
      <br>
      In order to address this issue, I increased the distance at which the robot would determine there was a wall in front of it to .7. Essentially, then the robot could measure a hypotenuse rather than the direct perpendicular distance to the wall. This meant the robot couldn't get as close to the wall, but it heavily reduced the number of collisions. 
      <br>
      <video width="320" height="240" controls>
      <source src="obstacle_avoidance.mp4" type="video/mp4">
      </video>
      <br>
      I continued tweaking the values for how long the robot should delay and how long it should turn for, and at what speeds. I found a combination of moving at .5 linear velocity for movement and 2 angular velocity for turns to be the best balance of speed to obstacle avoidance. In reality, I hope that the robot can glance off surfaces at a low enough angle without causing it to stop or flip, although this would not be ideal. 
      There is a delay of .1s between updates of this function, which will be adjusted as other functions need to occur between measurements. Once the robot is performing other tasks, those other tasks will take some finite amount of time, and that will have to cocur between measurements. 
      <br>
      The robot could move faster if this delay was decreased. 
      As seen in the video, the robot gets very very close, almost touching the wall at one point, before it deflects, not resulting in a collision. 
      In order to minimize crashes we could move slower, take measurements of wall distance more often, or set a larger distance required between the robot and an obstacle, which would work well in a  low-obstacle environment. 

      <code><pre>
      def perform_obstacle_avoidance(robot):
        while True: 

          robot.set_vel(0, 0)

          if (robot.get_laser_data() < .7): 
            robot.set_vel(0,2)
            time.sleep(.4)

          else: 
            robot.set_vel(.5, 0)
            time.sleep(.1)

      perform_obstacle_avoidance(robot)

      </pre></code>


    </p>
    <br>
    
  
 
   <a href="#header"><button class="btn"><i class="fa fa-arrow-up"></i>Return to top</button></a>
   </body>
</html>
